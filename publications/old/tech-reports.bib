@article{amos2019differentiable,
  title={{The Differentiable Cross-Entropy Method}},
  author={Amos, Brandon and Yarats, Denis},
  journal={arXiv preprint arXiv:1909.12830},
  year={2019},
  _venue={arXiv},
  url={https://arxiv.org/abs/1909.12830},
  abstract={
    We study the Cross-Entropy Method (CEM) for the non-convex
    optimization of a continuous and parameterized
    objective function and introduce a differentiable
    variant (DCEM) that enables us to differentiate the
    output of CEM with respect to the objective
    function's parameters. In the machine learning
    setting this brings CEM inside of the end-to-end
    learning pipeline where this has otherwise been
    impossible. We show applications in a synthetic
    energy-based structured prediction task and in
    non-convex continuous control. In the control
    setting we show on the simulated cheetah and walker
    tasks that we can embed their optimal action
    sequences with DCEM and then use policy optimization
    to fine-tune components of the controller as a step
    towards combining model-based and model-free RL.
  }
}

@article{grefenstette2019generalized,
  title={{Generalized Inner Loop Meta-Learning}},
  author={Grefenstette, Edward and Amos, Brandon and Yarats, Denis and Htut, Phu Mon and Molchanov, Artem and Meier, Franziska and Kiela, Douwe and Cho, Kyunghyun and Chintala, Soumith},
  journal={arXiv preprint arXiv:1910.01727},
  year={2019},
  _venue={arXiv},
  url={https://arxiv.org/abs/1910.01727},
  abstract={
    Many (but not all) approaches self-qualifying as "meta-learning" in
    deep learning and reinforcement learning fit a
    common pattern of approximating the solution to a
    nested optimization problem. In this paper, we give
    a formalization of this shared pattern, which we
    call GIMLI, prove its general requirements, and
    derive a general-purpose algorithm for implementing
    similar approaches. Based on this analysis and
    algorithm, we describe a library of our design,
    higher, which we share with the community to assist
    and enable future research into these kinds of
    meta-learning approaches. We end the paper by
    showcasing the practical applications of this
    framework and library through illustrative
    experiments and ablation studies which they
    facilitate.
  }
}

@article{yarats2019improving,
  title={{Improving Sample Efficiency in Model-Free Reinforcement Learning from Images}},
  author={Yarats, Denis and Zhang, Amy and Kostrikov, Ilya and Amos, Brandon and Pineau, Joelle and Fergus, Rob},
  journal={arXiv preprint arXiv:1910.01741},
  year={2019},
  _venue={arXiv},
  url={https://arxiv.org/abs/1910.01741},
  abstract={
    Training an agent to solve control tasks directly from
    high-dimensional images with model-free
    reinforcement learning (RL) has proven
    difficult. The agent needs to learn a latent
    representation together with a control policy to
    perform the task. Fitting a high-capacity encoder
    using a scarce reward signal is not only sample
    inefficient, but also prone to suboptimal
    convergence. Two ways to improve sample efficiency
    are to extract relevant features for the task and
    use off-policy algorithms. We dissect various
    approaches of learning good latent features, and
    conclude that the image reconstruction loss is the
    essential ingredient that enables efficient and
    stable representation learning in image-based
    RL. Following these findings, we devise an
    off-policy actor-critic algorithm with an auxiliary
    decoder that trains end-to-end and matches
    state-of-the-art performance across both model-free
    and model-based algorithms on many challenging
    control tasks. We release our code to encourage
    future research on image-based RL.
  }
}

@article{amos2019limited,
  title={{The Limited Multi-Label Projection Layer}},
  author={Brandon Amos and Vladlen Koltun and J. Zico Kolter},
  journal={arXiv preprint arXiv:1906.08707},
  year={2019},
  _venue={arXiv},
  url={https://arxiv.org/abs/1906.08707},
  codeurl={https://github.com/locuslab/lml},
  abstract={
    We propose the Limited Multi-Label (LML) projection layer as a new
    primitive operation for end-to-end learning systems. The LML layer
    provides a probabilistic way of modeling multi-label predictions
    limited to having exactly k labels. We derive efficient forward and
    backward passes for this layer and show how the layer can be used to
    optimize the top-k recall for multi-label tasks with incomplete label
    information. We evaluate LML layers on top-k CIFAR-100 classification
    and scene graph generation. We show that LML layers add a negligible
    amount of computational overhead, strictly improve the model's
    representational capacity, and improve accuracy. We also revisit the
    truncated top-k entropy method as a competitive baseline for top-k
    classification.
  }
}

@phdthesis{amos2019differentiable,
  author       = {Brandon Amos},
  title        = {{Differentiable Optimization-Based Modeling for Machine Learning}},
  school       = {Carnegie Mellon University},
  year         = 2019,
  month        = May,
  _venue = {Ph.D. Thesis},
  codeurl={https://github.com/bamos/thesis},
  url={https://github.com/bamos/thesis/raw/master/bamos_thesis.pdf},
}

@techreport{amos2016openface,
  title={OpenFace: A general-purpose face recognition
    library with mobile applications},
  author={Amos, Brandon and Bartosz Ludwiczuk and Satyanarayanan, Mahadev},
  _venue={CMU},
  year={2016},
  institution={Technical Report CMU-CS-16-118, CMU School of Computer Science},
  url={http://reports-archive.adm.cs.cmu.edu/anon/anon/2016/CMU-CS-16-118.pdf},
  codeurl={https://cmusatyalab.github.io/openface},
  abstract={
    Cameras are becoming ubiquitous in the Internet of Things (IoT) and
    can use face recognition technology to improve context. There is a
    large accuracy gap between today's publicly available face recognition
    systems and the state-of-the-art private face recognition
    systems. This paper presents our OpenFace face recognition library
    that bridges this accuracy gap. We show that OpenFace provides
    near-human accuracy on the LFW benchmark and present a new
    classification benchmark for mobile scenarios. This paper is intended
    for non-experts interested in using OpenFace and provides a light
    introduction to the deep neural network techniques we use.

    We released OpenFace in October 2015 as an open source library under
    the Apache 2.0 license. It is available at:
    <http://cmusatyalab.github.io/openface/>
  }
}

@techreport{gao2015cloudlets,
  title={Are Cloudlets Necessary?},
  author={
    Gao, Ying and Hu, Wenlu and Ha, Kiryong and
    Amos, Brandon and Pillai, Padmanabhan and
    Satyanarayanan, Mahadev
  },
  _venue={CMU},
  year={2015},
  institution={Technical Report CMU-CS-15-139, CMU School of Computer Science},
  url={http://reports-archive.adm.cs.cmu.edu/anon/anon/2015/CMU-CS-15-139.pdf},
  abstract={
    We present experimental results from Wi-Fi and 4G LTE networks to validate the
    intuition that low end-to-end latency of cloud services improves application
    response time and reduces energy consumption on mobile devices. We focus
    specifically on computational offloading as a cloud service. Using a wide
    range of applications, and exploring both pre-partitioned and dynamically
    partitioned approaches, we demonstrate the importance of low latency for
    cloud offload services. We show the best performance is achieved by
    offloading to cloudlets, which are small-scale edge-located data centers. Our
    results show that cloudlets can improve response times 51\% and reduce energy
    consumption in a mobile device by up to 42\% compared to cloud offload.
  }
}

@techreport{ha2015adaptive,
  title={Adaptive VM handoff across cloudlets},
  author={
    Ha, Kiryong and Abe, Yoshihisa and Chen, Zhuo and
    Hu, Wenlu and Amos, Brandon and Pillai, Padmanabhan and
    Satyanarayanan, Mahadev
  },
  _venue={CMU},
  year={2015},
  institution={Technical Report CMU-CS-15-113, CMU School of Computer Science},
  url={http://ra.adm.cs.cmu.edu/anon/2015/CMU-CS-15-113.pdf},
  abstract={
    Cloudlet offload is a valuable technique for ensuring low end-to-end latency of
    resource-intensive cloud processing for many emerging mobile applications.
    This paper examines the impact of user mobility on cloudlet offload, and
    shows that even modest user mobility can result in significant network
    degradation. We propose VM handoff as a technique for seamlessly transferring
    VMencapsulated execution to a more optimal offload site as users move. Our
    approach can perform handoff in roughly a minute even over limited WANs by
    adaptively reducing data transferred. We present experimental results to
    validate our implementation and to demonstrate effectiveness of adaptation to
    changing network conditions and processing capacity
  }
}

@article{amos2014QNSTOP,
  title={{{QNSTOP-QuasiNewton Algorithm for Stochastic Optimization}}},
  author={Brandon Amos and David Easterling and Layne Watson and
    William Thacker and Brent Castle and Michael Trosset},
  journal={},
  _venue={VT},
  year={2014},
  keywords={journal},
  url={https://vtechworks.lib.vt.edu/bitstream/handle/10919/49672/qnTOMS14.pdf},
  abstract={
    QNSTOP consists of serial and parallel (OpenMP) Fortran 2003 codes for the
    quasi-Newton stochastic optimization method of Castle and Trosset. For
    stochastic problems, convergence theory exists for the particular
    algorithmic choices and parameter values used in QNSTOP. Both the parallel
    driver subroutine, which offers several parallel decomposition strategies,
    and the serial driver subroutine can be used for stochastic optimization or
    deterministic global optimization, based on an input switch. QNSTOP is
    particularly effective for “noisy” deterministic problems, using only
    objective function values. Some performance data for computational systems
    biology problems is given.
  }
}
